{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhl9W5p91gn-",
        "outputId": "36fc608a-e19e-471a-c227-a750d0877204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BFS Crawler\n",
            "Visiting: home\n",
            "Visiting: about\n",
            "Visiting: services\n",
            "Visiting: contact\n",
            "Visiting: team\n",
            "Visiting: mission\n",
            "Visiting: web_dev\n",
            "Visiting: data_science\n",
            "\n",
            "DFS Crawler\n",
            "Visiting: home\n",
            "Visiting: about\n",
            "Visiting: team\n",
            "Visiting: mission\n",
            "Visiting: services\n",
            "Visiting: web_dev\n",
            "Visiting: data_science\n",
            "Visiting: contact\n",
            "\n",
            "Focused Crawler\n",
            "Visiting: data_science (Focused Search)\n",
            "\n",
            "Incremental Crawler\n",
            "Visiting: home (New page)\n",
            "Visiting: services (New page)\n",
            "Visiting: contact (New page)\n",
            "Visiting: web_dev (New page)\n",
            "Visiting: data_science (New page)\n",
            "\n",
            "Parallel Crawler\n",
            "\n",
            "BFS Crawler\n",
            "Visiting: home\n",
            "\n",
            "BFS Crawler\n",
            "Visiting: services\n",
            "Visiting: about\n",
            "Visiting: web_dev\n",
            "Visiting: services\n",
            "Visiting: data_science\n",
            "Visiting: contact\n",
            "Visiting: team\n",
            "Visiting: mission\n",
            "Visiting: web_dev\n",
            "Visiting: data_science\n"
          ]
        }
      ],
      "source": [
        "from collections import deque\n",
        "import time\n",
        "from threading import Thread\n",
        "\n",
        "# Dummy function to simulate fetching web page links\n",
        "def fetch_links(url):\n",
        "    dummy_web = {\n",
        "        \"home\": [\"about\", \"services\", \"contact\"],\n",
        "        \"about\": [\"team\", \"mission\"],\n",
        "        \"services\": [\"web_dev\", \"data_science\"],\n",
        "        \"contact\": [],\n",
        "        \"team\": [],\n",
        "        \"mission\": [],\n",
        "        \"web_dev\": [],\n",
        "        \"data_science\": []\n",
        "    }\n",
        "    return dummy_web.get(url, [])\n",
        "\n",
        "# 1. Breadth-First Search (BFS) Crawler\n",
        "def bfs_crawler(start_url):\n",
        "    print(\"\\nBFS Crawler\")\n",
        "    queue = deque([start_url])\n",
        "    visited = set()\n",
        "\n",
        "    while queue:\n",
        "        url = queue.popleft()\n",
        "        if url not in visited:\n",
        "            print(f\"Visiting: {url}\")\n",
        "            visited.add(url)\n",
        "            queue.extend(fetch_links(url))\n",
        "            time.sleep(1)\n",
        "\n",
        "# 2. Depth-First Search (DFS) Crawler\n",
        "def dfs_crawler(start_url, visited=None):\n",
        "    if visited is None:\n",
        "        visited = set()\n",
        "        print(\"\\nDFS Crawler\")\n",
        "\n",
        "    if start_url not in visited:\n",
        "        print(f\"Visiting: {start_url}\")\n",
        "        visited.add(start_url)\n",
        "        time.sleep(1)\n",
        "\n",
        "        for link in fetch_links(start_url):\n",
        "            dfs_crawler(link, visited)\n",
        "\n",
        "# 3. Focused Crawler (only searches pages with \"data\" in the name)\n",
        "def focused_crawler(start_url, keyword):\n",
        "    print(\"\\nFocused Crawler\")\n",
        "    queue = deque([start_url])\n",
        "    visited = set()\n",
        "\n",
        "    while queue:\n",
        "        url = queue.popleft()\n",
        "        if url not in visited:\n",
        "            visited.add(url)\n",
        "            if keyword in url:\n",
        "                print(f\"Visiting: {url} (Focused Search)\")\n",
        "            queue.extend(fetch_links(url))  # Continue exploring child links\n",
        "            time.sleep(1)\n",
        "\n",
        "# 4. Incremental Crawler (Only visits new/updated pages)\n",
        "def incremental_crawler(start_url, last_visited):\n",
        "    print(\"\\nIncremental Crawler\")\n",
        "    queue = deque([start_url])\n",
        "    visited = set()\n",
        "\n",
        "    while queue:\n",
        "        url = queue.popleft()\n",
        "        if url not in visited and url not in last_visited:\n",
        "            print(f\"Visiting: {url} (New page)\")\n",
        "            visited.add(url)\n",
        "            queue.extend(fetch_links(url))\n",
        "            time.sleep(1)\n",
        "\n",
        "# 5. Parallel Crawler (Simulated with multiple threads)\n",
        "def parallel_worker(start_url):\n",
        "    bfs_crawler(start_url)\n",
        "\n",
        "def parallel_crawler(start_urls):\n",
        "    print(\"\\nParallel Crawler\")\n",
        "    threads = []\n",
        "    for url in start_urls:\n",
        "        thread = Thread(target=parallel_worker, args=(url,))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "# Testing the crawlers\n",
        "bfs_crawler(\"home\")\n",
        "dfs_crawler(\"home\")\n",
        "focused_crawler(\"home\", \"data\")\n",
        "incremental_crawler(\"home\", {\"about\", \"team\"})\n",
        "parallel_crawler([\"home\", \"services\"])\n"
      ]
    }
  ]
}